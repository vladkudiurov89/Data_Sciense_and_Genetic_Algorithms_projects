{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_start_end_values.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClRwY2_guZLG"
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1zZIE7eunpE"
      },
      "source": [
        "# Install the dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJQOO_eSunr5"
      },
      "source": [
        "# Set the environment variables for running PySpark in the collaboration environmentimport os\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "gByYsDNMunu9",
        "outputId": "7e08c143-a30e-42f2-cbdf-a0a24c31ae4c"
      },
      "source": [
        "# Run the local session to test the installation\n",
        "import findspark\n",
        "findspark.init('spark-3.0.1-bin-hadoop3.2')\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://908c016e60a0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff55c0fc1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOt4FecYunxs"
      },
      "source": [
        "# Create dataframe\n",
        "projects = spark.createDataFrame(\n",
        "                              [ (1, '2020-01-01', '2020-01-02'),\n",
        "                                (2, '2020-01-02', '2020-01-03'),\n",
        "                                (3, '2020-01-03', '2020-01-04'),\n",
        "                                (4, '2020-01-04', '2020-01-05'),\n",
        "                                (5, '2020-01-06', '2020-01-07'),\n",
        "                                (6, '2020-01-16', '2020-01-17'),\n",
        "                                (7, '2020-01-17', '2020-01-18'),\n",
        "                                (8, '2020-01-18', '2020-01-19'),\n",
        "                                (9, '2020-01-19', '2020-01-20'),\n",
        "                                (10, '2020-01-21', '2020-01-22'),\n",
        "                                (11, '2020-01-26', '2020-01-27'),\n",
        "                                (12, '2020-01-27', '2020-01-28'),\n",
        "                                (13, '2020-01-28', '2020-01-29'),\n",
        "                                (14, '2020-01-29', '2020-01-30'),],\n",
        "                              ['proj_id' , 'proj_start', 'proj_end'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhIOLpEzvNJM",
        "outputId": "f994a9f2-6a25-47d3-8bc4-d5fcc2db01cb"
      },
      "source": [
        "projects.show()\n",
        "projects.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+\n",
            "|proj_id|proj_start|  proj_end|\n",
            "+-------+----------+----------+\n",
            "|      1|2020-01-01|2020-01-02|\n",
            "|      2|2020-01-02|2020-01-03|\n",
            "|      3|2020-01-03|2020-01-04|\n",
            "|      4|2020-01-04|2020-01-05|\n",
            "|      5|2020-01-06|2020-01-07|\n",
            "|      6|2020-01-16|2020-01-17|\n",
            "|      7|2020-01-17|2020-01-18|\n",
            "|      8|2020-01-18|2020-01-19|\n",
            "|      9|2020-01-19|2020-01-20|\n",
            "|     10|2020-01-21|2020-01-22|\n",
            "|     11|2020-01-26|2020-01-27|\n",
            "|     12|2020-01-27|2020-01-28|\n",
            "|     13|2020-01-28|2020-01-29|\n",
            "|     14|2020-01-29|2020-01-30|\n",
            "+-------+----------+----------+\n",
            "\n",
            "root\n",
            " |-- proj_id: long (nullable = true)\n",
            " |-- proj_start: string (nullable = true)\n",
            " |-- proj_end: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lrVeOcwvNMB"
      },
      "source": [
        "# Change type from string to date\n",
        "from datetime import datetime\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import *\n",
        "udf_date = udf(lambda x:datetime.strptime(x, \"%Y-%m-%d\"),DateType())\n",
        "df = projects.withColumn('proj_start',udf_date(col('proj_start'))).withColumn('proj_end',udf_date(col('proj_end')))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CxK10XmvNPA",
        "outputId": "41d98e34-dbac-4fe2-dfb6-917a62348e3e"
      },
      "source": [
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+\n",
            "|proj_id|proj_start|  proj_end|\n",
            "+-------+----------+----------+\n",
            "|      1|2020-01-01|2020-01-02|\n",
            "|      2|2020-01-02|2020-01-03|\n",
            "|      3|2020-01-03|2020-01-04|\n",
            "|      4|2020-01-04|2020-01-05|\n",
            "|      5|2020-01-06|2020-01-07|\n",
            "|      6|2020-01-16|2020-01-17|\n",
            "|      7|2020-01-17|2020-01-18|\n",
            "|      8|2020-01-18|2020-01-19|\n",
            "|      9|2020-01-19|2020-01-20|\n",
            "|     10|2020-01-21|2020-01-22|\n",
            "|     11|2020-01-26|2020-01-27|\n",
            "|     12|2020-01-27|2020-01-28|\n",
            "|     13|2020-01-28|2020-01-29|\n",
            "|     14|2020-01-29|2020-01-30|\n",
            "+-------+----------+----------+\n",
            "\n",
            "root\n",
            " |-- proj_id: long (nullable = true)\n",
            " |-- proj_start: date (nullable = true)\n",
            " |-- proj_end: date (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s23eGoIpvNR9"
      },
      "source": [
        "# Spark SQL\n",
        "df.createOrReplaceTempView(\"tbl\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc8soFB_vydP"
      },
      "source": [
        "# Query\n",
        "df_sql = spark.sql(\"\"\"\n",
        "                      select \n",
        "                            p3.proj_group, \n",
        "                            min(p3.proj_start) as date_start,\n",
        "                            max(p3.proj_end) as date_end,\n",
        "                            datediff(max(p3.proj_end), min(p3.proj_end))+1 as delta\n",
        "                      from\n",
        "                          (select \n",
        "                            p2.*,\n",
        "                            sum(p2.flag)over(order by p2.proj_id) as proj_group\n",
        "                        from \n",
        "                          (select \n",
        "                                p.proj_id , \n",
        "                                p.proj_start, \n",
        "                                p.proj_end, \n",
        "                                case \n",
        "                                when lag(p.proj_end)over(order by p.proj_id) = p.proj_start then 0 else 1 \n",
        "                                end as flag\n",
        "                          from tbl as p) as p2) as p3\n",
        "                      group by p3.proj_group\n",
        "                    \"\"\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEFcokuGvygF",
        "outputId": "4eb9f38f-91eb-4b62-dd48-610c688db534"
      },
      "source": [
        "df_sql.show()\n",
        "df_sql.printSchema()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+-----+\n",
            "|proj_group|date_start|  date_end|delta|\n",
            "+----------+----------+----------+-----+\n",
            "|         1|2020-01-01|2020-01-05|    4|\n",
            "|         2|2020-01-06|2020-01-07|    1|\n",
            "|         3|2020-01-16|2020-01-20|    4|\n",
            "|         4|2020-01-21|2020-01-22|    1|\n",
            "|         5|2020-01-26|2020-01-30|    4|\n",
            "+----------+----------+----------+-----+\n",
            "\n",
            "root\n",
            " |-- proj_group: long (nullable = true)\n",
            " |-- date_start: date (nullable = true)\n",
            " |-- date_end: date (nullable = true)\n",
            " |-- delta: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAt8nGaIvyjH",
        "outputId": "c219b5c9-0f7d-46b4-ccf3-949a4b2bb82c"
      },
      "source": [
        "# Spark DataFrame\n",
        "from pyspark.sql.functions import lag\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "# Equivalent of Pandas.dataframe.shift() method\n",
        "w = Window().partitionBy().orderBy(col(\"proj_id\"))\n",
        "df_dataframe = df.withColumn('lag', F.lag(\"proj_end\").over(w))\n",
        "df_dataframe.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+----------+\n",
            "|proj_id|proj_start|  proj_end|       lag|\n",
            "+-------+----------+----------+----------+\n",
            "|      1|2020-01-01|2020-01-02|      null|\n",
            "|      2|2020-01-02|2020-01-03|2020-01-02|\n",
            "|      3|2020-01-03|2020-01-04|2020-01-03|\n",
            "|      4|2020-01-04|2020-01-05|2020-01-04|\n",
            "|      5|2020-01-06|2020-01-07|2020-01-05|\n",
            "|      6|2020-01-16|2020-01-17|2020-01-07|\n",
            "|      7|2020-01-17|2020-01-18|2020-01-17|\n",
            "|      8|2020-01-18|2020-01-19|2020-01-18|\n",
            "|      9|2020-01-19|2020-01-20|2020-01-19|\n",
            "|     10|2020-01-21|2020-01-22|2020-01-20|\n",
            "|     11|2020-01-26|2020-01-27|2020-01-22|\n",
            "|     12|2020-01-27|2020-01-28|2020-01-27|\n",
            "|     13|2020-01-28|2020-01-29|2020-01-28|\n",
            "|     14|2020-01-29|2020-01-30|2020-01-29|\n",
            "+-------+----------+----------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ZIn4J2vyl6",
        "outputId": "b0d61bb6-490f-4f22-d91e-48c09c4c0c99"
      },
      "source": [
        "# Equivalent of SQL- CASE WHEN...THEN...ELSE... END\n",
        "df_dataframe = df_dataframe.withColumn('flag',F.when(df_dataframe[\"proj_start\"] == df_dataframe[\"lag\"],0).otherwise(1))\n",
        "df_dataframe.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+----------+----+\n",
            "|proj_id|proj_start|  proj_end|       lag|flag|\n",
            "+-------+----------+----------+----------+----+\n",
            "|      1|2020-01-01|2020-01-02|      null|   1|\n",
            "|      2|2020-01-02|2020-01-03|2020-01-02|   0|\n",
            "|      3|2020-01-03|2020-01-04|2020-01-03|   0|\n",
            "|      4|2020-01-04|2020-01-05|2020-01-04|   0|\n",
            "|      5|2020-01-06|2020-01-07|2020-01-05|   1|\n",
            "|      6|2020-01-16|2020-01-17|2020-01-07|   1|\n",
            "|      7|2020-01-17|2020-01-18|2020-01-17|   0|\n",
            "|      8|2020-01-18|2020-01-19|2020-01-18|   0|\n",
            "|      9|2020-01-19|2020-01-20|2020-01-19|   0|\n",
            "|     10|2020-01-21|2020-01-22|2020-01-20|   1|\n",
            "|     11|2020-01-26|2020-01-27|2020-01-22|   1|\n",
            "|     12|2020-01-27|2020-01-28|2020-01-27|   0|\n",
            "|     13|2020-01-28|2020-01-29|2020-01-28|   0|\n",
            "|     14|2020-01-29|2020-01-30|2020-01-29|   0|\n",
            "+-------+----------+----------+----------+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz2C87Qqw6rQ",
        "outputId": "cc5122c0-4019-4295-c7df-70a277a616b4"
      },
      "source": [
        "# Cumsum by column flag\n",
        "w = Window().partitionBy().orderBy(col(\"proj_id\"))\n",
        "df_dataframe = df_dataframe.withColumn(\"proj_group\", F.sum(\"flag\").over(w))\n",
        "df_dataframe.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+----------+----+----------+\n",
            "|proj_id|proj_start|  proj_end|       lag|flag|proj_group|\n",
            "+-------+----------+----------+----------+----+----------+\n",
            "|      1|2020-01-01|2020-01-02|      null|   1|         1|\n",
            "|      2|2020-01-02|2020-01-03|2020-01-02|   0|         1|\n",
            "|      3|2020-01-03|2020-01-04|2020-01-03|   0|         1|\n",
            "|      4|2020-01-04|2020-01-05|2020-01-04|   0|         1|\n",
            "|      5|2020-01-06|2020-01-07|2020-01-05|   1|         2|\n",
            "|      6|2020-01-16|2020-01-17|2020-01-07|   1|         3|\n",
            "|      7|2020-01-17|2020-01-18|2020-01-17|   0|         3|\n",
            "|      8|2020-01-18|2020-01-19|2020-01-18|   0|         3|\n",
            "|      9|2020-01-19|2020-01-20|2020-01-19|   0|         3|\n",
            "|     10|2020-01-21|2020-01-22|2020-01-20|   1|         4|\n",
            "|     11|2020-01-26|2020-01-27|2020-01-22|   1|         5|\n",
            "|     12|2020-01-27|2020-01-28|2020-01-27|   0|         5|\n",
            "|     13|2020-01-28|2020-01-29|2020-01-28|   0|         5|\n",
            "|     14|2020-01-29|2020-01-30|2020-01-29|   0|         5|\n",
            "+-------+----------+----------+----------+----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXGv95ww6uR",
        "outputId": "b8f21286-4062-40c5-90fa-2781262f0f37"
      },
      "source": [
        "# Equivalent of SQL - GROUP BY\n",
        "from pyspark.sql.functions import  min, max\n",
        "df_group = df_dataframe.groupBy(\"proj_group\").agg(min(\"proj_start\").alias(\"date_start\"), \\\n",
        "                                                  max(\"proj_end\").alias(\"date_end\"))\n",
        "df_group = df_group.withColumn(\"delta\", F.datediff(df_group.date_end,df_group.date_start))\n",
        "df_group.show()\n",
        "df_group.printSchema()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+-----+\n",
            "|proj_group|date_start|  date_end|delta|\n",
            "+----------+----------+----------+-----+\n",
            "|         1|2020-01-01|2020-01-05|    4|\n",
            "|         2|2020-01-06|2020-01-07|    1|\n",
            "|         3|2020-01-16|2020-01-20|    4|\n",
            "|         4|2020-01-21|2020-01-22|    1|\n",
            "|         5|2020-01-26|2020-01-30|    4|\n",
            "+----------+----------+----------+-----+\n",
            "\n",
            "root\n",
            " |-- proj_group: long (nullable = true)\n",
            " |-- date_start: date (nullable = true)\n",
            " |-- date_end: date (nullable = true)\n",
            " |-- delta: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}