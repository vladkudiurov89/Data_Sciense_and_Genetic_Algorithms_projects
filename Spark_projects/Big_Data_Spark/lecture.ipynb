{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мотивация создания Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассмотрим два примера приложений:\n",
    "- Обучить модель на больших данных (читай итеративный алгоритм над фиксированным датасетом)\n",
    "- Провести ad-hoc анализ данных из двух таблиц (читай несколько интерактивных запросов с джойнами)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные недостатки классического MapReduce\n",
    "- Быстроумирающие контейнеры\n",
    "- Постоянное чтение/запись во внешнее хранилище\n",
    "- Сложный API\n",
    "- Ограниченное число источников/приемников данных\n",
    "- MapReduce - это только вычислительный фреймворк"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark - это *быстрая* распределенная вычислительная платформа *общего назначения*\n",
    "1. **Быстрая** - это в памяти и с ленивыми вычислениями\n",
    "2. **Общего назначения** - значит на ней можно реализовать любые вычисления (батчевые, интерактивные, итеративные, в режиме реального времени)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/spark_stack.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Множество источников данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/spark_data_sources.jpg\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/cluster-overview.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext (sc) - это основной управляющий объект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://brain-client.bigdatateam.org:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для получения всех установленных опций конфигурации можно использовать `sc.getConf()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.eventLog.dir', 'hdfs://brain-master.bigdatateam.org/spark/logs'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '12'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1602972439113_9908'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'brain-master.bigdatateam.org'),\n",
       " ('spark.driver.host', 'brain-client.bigdatateam.org'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'hdfs://brain-master.bigdatateam.org/spark/logs'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.appUIAddress', 'http://brain-client.bigdatateam.org:4040'),\n",
       " ('spark.yarn.historyServer.address', 'brain-master.bigdatateam.org:18080'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.app.id', 'application_1602972439113_9908'),\n",
       " ('spark.driver.memory', '512m'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.driver.port', '37415'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/local/spark/python/lib/py4j-0.10.7-src.zip:/usr/local/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.executor.cores', '1'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://brain-master.bigdatateam.org:8088/proxy/application_1602972439113_9908'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Существует два способа создать RDD\n",
    "- распределить коллекцию объектов с драйвера\n",
    "- загрузить внешний датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Распределить коллекцию с драйвера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocabulary = (\"Apache\", \"Spark\", \"Hadoop\")\n",
    "numbers = np.random.randint(10, size=10000)\n",
    "words = np.random.choice(vocabulary, size=10000)\n",
    "collection = zip(numbers, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 'Spark'),\n",
       " (9, 'Hadoop'),\n",
       " (3, 'Spark'),\n",
       " (7, 'Hadoop'),\n",
       " (5, 'Spark'),\n",
       " (4, 'Apache'),\n",
       " (8, 'Apache'),\n",
       " (6, 'Spark'),\n",
       " (6, 'Apache'),\n",
       " (9, 'Spark')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузить внешний датасет (датасет загружается из HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 hdfs hdfs      19462 2020-08-11 20:22 /data/spark/lecture05/ips.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /data/spark/lecture05/ips.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.textFile(\"/data/spark/lecture05/ips.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['192.168.0.1\\tCHINA',\n",
       " '192.168.0.2\\tCHINA',\n",
       " '192.168.0.3\\tCHINA',\n",
       " '192.168.0.4\\tCHINA',\n",
       " '192.168.0.5\\tCHINA',\n",
       " '192.168.0.6\\tCHINA',\n",
       " '192.168.0.7\\tCHINA',\n",
       " '192.168.0.8\\tCHINA',\n",
       " '192.168.0.9\\tCHINA',\n",
       " '192.168.0.10\\tCHINA']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD API состоит из операции двух типов:\n",
    "- action\n",
    "- transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформация преобразовывает RDD в другой RDD и не приводит к вычислениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action заставляет Spark произвести вычисления и вернуть результат либо на драйвер, либо во внешнее хранилище"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации можно применять одну за другой, никаких вычислений не будет сделано, пока не будет вызван action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.filter(lambda x: x % 2)\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[18] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: x * 2)\n",
    "rdd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 6,\n",
       " 10,\n",
       " 14,\n",
       " 18,\n",
       " 22,\n",
       " 26,\n",
       " 30,\n",
       " 34,\n",
       " 38,\n",
       " 42,\n",
       " 46,\n",
       " 50,\n",
       " 54,\n",
       " 58,\n",
       " 62,\n",
       " 66,\n",
       " 70,\n",
       " 74,\n",
       " 78,\n",
       " 82,\n",
       " 86,\n",
       " 90,\n",
       " 94,\n",
       " 98,\n",
       " 102,\n",
       " 106,\n",
       " 110,\n",
       " 114,\n",
       " 118,\n",
       " 122,\n",
       " 126,\n",
       " 130,\n",
       " 134,\n",
       " 138,\n",
       " 142,\n",
       " 146,\n",
       " 150,\n",
       " 154,\n",
       " 158,\n",
       " 162,\n",
       " 166,\n",
       " 170,\n",
       " 174,\n",
       " 178,\n",
       " 182,\n",
       " 186,\n",
       " 190,\n",
       " 194,\n",
       " 198]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как получить данные на драйвер и не выстрелить себе в ногу?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `take()` пытается минимизировать число обращений к партициям, поэтому может возвращать смещенные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будьте аккуратны с `collect()`, потому что он загружает все данные из RDD на драйвер. Это может легко привести к `Out of Memory exception`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Если нужно получить небольшое число записей на драйвер и, при этом, сохранить распределение, то лучше сделать выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 75, 57, 18, 19, 96, 11, 55, 39, 40, 79, 26, 15, 13, 47, 7, 66, 86, 51, 12]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(withReplacement=False, num=20, seed=5757)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Познакомимся с данными. Будем работать с двумя таблицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/data_table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/data_table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры трансформаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/spark/lecture05/ips.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['192.168.0.1\\tCHINA',\n",
       " '192.168.0.2\\tCHINA',\n",
       " '192.168.0.3\\tCHINA',\n",
       " '192.168.0.4\\tCHINA',\n",
       " '192.168.0.5\\tCHINA']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips = rdd.map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.1', 'CHINA'],\n",
       " ['192.168.0.2', 'CHINA'],\n",
       " ['192.168.0.3', 'CHINA'],\n",
       " ['192.168.0.4', 'CHINA'],\n",
       " ['192.168.0.5', 'CHINA']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_filtered = ips.filter(lambda x: x[1] != \"CHINA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_filtered.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_logs = sc.textFile(\"/data/spark/lecture05/log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = raw_logs.map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs.flatMap(lambda x: x[2].split()).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = logs.flatMap(lambda x: x[2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(lambda word: (word, 1)).countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны отдельные трансформации и отдельные action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/dag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/dag2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Последовательность трансформаций определяет граф вычислений (DAG - direct acyclic graph). В нем есть партиции и зависимости между партициями. Таким образом Spark имеет всю необходимую информацию для вычилсения графа в любой точке и возможных оптимизаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/dag3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации бывают *узкими*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/narrow_transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И *широкими*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/wide_transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Широкие трансформации разделяют джоб на стейджи. Между стейджами происходит shuffle данных, которого надо избегать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Персистентность и кэширование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD вычисляются лениво, когда вызывается action. Часто мы хотим вызвать несколько actions для одного и тоге же RDD. Если мы просто сделаем это, то граф будет полностью перевычисляться каждый раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.3.99', 'USA'],\n",
       " ['192.168.3.98', 'USA'],\n",
       " ['192.168.3.97', 'USA'],\n",
       " ['192.168.3.96', 'USA'],\n",
       " ['192.168.3.95', 'USA'],\n",
       " ['192.168.3.94', 'USA'],\n",
       " ['192.168.3.93', 'USA'],\n",
       " ['192.168.3.92', 'USA'],\n",
       " ['192.168.3.91', 'USA'],\n",
       " ['192.168.3.90', 'USA']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтобы этого избежать, мы можем закэшировать RDD в памяти. Кэширование произойдет при вызове первого action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_cached = ips.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[28] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.3.99', 'USA'],\n",
       " ['192.168.3.98', 'USA'],\n",
       " ['192.168.3.97', 'USA'],\n",
       " ['192.168.3.96', 'USA'],\n",
       " ['192.168.3.95', 'USA'],\n",
       " ['192.168.3.94', 'USA'],\n",
       " ['192.168.3.93', 'USA'],\n",
       " ['192.168.3.92', 'USA'],\n",
       " ['192.168.3.91', 'USA'],\n",
       " ['192.168.3.90', 'USA'],\n",
       " ['192.168.3.9', 'USA'],\n",
       " ['192.168.3.89', 'USA'],\n",
       " ['192.168.3.88', 'USA'],\n",
       " ['192.168.3.87', 'USA'],\n",
       " ['192.168.3.86', 'USA'],\n",
       " ['192.168.3.85', 'USA'],\n",
       " ['192.168.3.84', 'USA'],\n",
       " ['192.168.3.83', 'USA'],\n",
       " ['192.168.3.82', 'USA'],\n",
       " ['192.168.3.81', 'USA']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips_cached.top(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0 K  /data/spark/lecture05/ips.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -s -h /data/spark/lecture05/ips.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cache()` сохраняет RDD в памяти. Для большего контроля можно использовать `persist(storage_level)`:\n",
    "+ MEMORY_ONLY\n",
    "+ MEMORY_AND_DISK\n",
    "+ DISK_ONLY\n",
    "+ MEMORY_ONLY_2\n",
    "+ MEMORY_AND_DISK_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все сохраненные RDD можно увидеть во вкладке \"Storage\" Spark UI\n",
    "### Или более программатичным способом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StorageLevel(False, True, False, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[28] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[28] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.persist(StorageLevel.DISK_ONLY_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, False, False, False, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairRDD (ключ-значение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PairRDD - это RDD для работы с парами ключ-значение. Spark предполагает, что PairRDD содержить в себе объекты, состящие ровно из двух элементов! PairRDD предоставляют методы группировки, аггрегации и объединения (join) двух RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пусть есть задача подсчитать распределение кодов ERROR и WARNING в лог-файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['192.168.0.10\\tERROR\\tWhen production fails in dipsair, whom you gonna call?',\n",
       " '192.168.0.39\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.35\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.19\\tINFO\\tJust an info message passing by',\n",
       " '192.168.0.23\\tERROR\\tWhen production fails in dipsair, whom you gonna call?']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ERROR', 49948), ('WARNING', 250004)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_logs.filter(lambda x: \"INFO\" not in x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[1], 1))\\\n",
    "         .groupByKey()\n",
    "         .map(lambda x: (x[0], len(x[1])))\n",
    "         .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Или немного проще"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('ERROR', 49948), ('WARNING', 250004)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_logs.filter(lambda x: \"INFO\" not in x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[1], 1))\n",
    "         .countByKey()\n",
    "         .items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоит заметить, что `groupByKey()` предполагает перемещение всех записей с одним ключом на один экзекьютор. В случае очень скоршенных распределений это может привести к падению экзекьютора с OOM. Поэтому всегда при группировках стоит подумать об использовании `reduceByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ERROR', 49948), ('WARNING', 250004)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_logs.filter(lambda x: \"INFO\" not in x)\n",
    "         .map(lambda x: (x.split(\"\\t\")[1], 1))\\\n",
    "         .reduceByKey(lambda x, y: x + y)\n",
    "         .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Два PairRDD можно объединить по ключу\n",
    "### Поддерживаются inner join, left outer join, right outer join и full outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.10',\n",
       "  'ERROR',\n",
       "  'When production fails in dipsair, whom you gonna call?'],\n",
       " ['192.168.0.39', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.35', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.19', 'INFO', 'Just an info message passing by'],\n",
       " ['192.168.0.23',\n",
       "  'ERROR',\n",
       "  'When production fails in dipsair, whom you gonna call?']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['192.168.0.1', 'CHINA'],\n",
       " ['192.168.0.2', 'CHINA'],\n",
       " ['192.168.0.3', 'CHINA'],\n",
       " ['192.168.0.4', 'CHINA'],\n",
       " ['192.168.0.5', 'CHINA']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('192.168.0.55', ('INFO', 'CHINA')),\n",
       " ('192.168.0.55', ('INFO', 'CHINA')),\n",
       " ('192.168.0.55', ('INFO', 'CHINA')),\n",
       " ('192.168.0.55', ('INFO', 'CHINA')),\n",
       " ('192.168.0.55', ('INFO', 'CHINA'))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.join(ips).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/Jackie-Chan-WTF.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не стоит забывать, что Spark предполагает, что PairRDD состоит ровно! из двух элементов, поэтому все остальные элементы просто отбрасываются!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_logs(line):\n",
    "    split = line.split(\"\\t\")\n",
    "    return split[0], split[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_cached = raw_logs.map(split_logs).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('192.168.0.10',\n",
       "  ['ERROR', 'When production fails in dipsair, whom you gonna call?']),\n",
       " ('192.168.0.39', ['INFO', 'Just an info message passing by']),\n",
       " ('192.168.0.35', ['INFO', 'Just an info message passing by']),\n",
       " ('192.168.0.19', ['INFO', 'Just an info message passing by']),\n",
       " ('192.168.0.23',\n",
       "  ['ERROR', 'When production fails in dipsair, whom you gonna call?'])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_cached.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('192.168.0.1', (['INFO', 'Just an info message passing by'], 'CHINA')),\n",
       " ('192.168.0.1', (['INFO', 'Just an info message passing by'], 'CHINA')),\n",
       " ('192.168.0.1', (['WARNING', 'Something bad could happen'], 'CHINA')),\n",
       " ('192.168.0.1', (['WARNING', 'Something bad could happen'], 'CHINA')),\n",
       " ('192.168.0.1', (['INFO', 'Just an info message passing by'], 'CHINA'))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_cached.join(ips).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Управление параллелизмом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомним, что атомарным уровнем параллелизма в Spark является партиция. Об этом всегда стоит помнить, когда есть проблемы с производительностью приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод `repartition()` может быть использован для изменения числа партиций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `repartition()` всегда приводит к равномерному перераспределению данных, что ведет к shuffle. Если Вы уменьшаете число партиций, то стоит использовать `coalesce()`, который может избежать shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.coalesce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.coalesce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Узнать дефолтный уровень параллелизма можно из конфига. По-умолчанию, при работе с YARN, использукется общее число ядер, выделенных этому SparkContext на всех экзекьюторах, либо 2. Что больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf().get(\"spark.default.parallelism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(100)).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast-объект - это неизменяемый объект, которая разделяется между всеми экзекьюторами\n",
    "### Дистрибуция broadcast-объекта производится быстро и эффективно p2p-протоколом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуем map-side join с помощью broadcast-объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_local = dict(ips.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_local['192.168.0.10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_broadcasted = sc.broadcast(ips_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_broadcasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_broadcasted.value['192.168.0.10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_cached.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_ip(row):\n",
    "    return ips_broadcasted.value[row[0]], row[1:] ## row[0] is the IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_cached.map(resolve_ip).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Не забудьте погасить SparkContext!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
