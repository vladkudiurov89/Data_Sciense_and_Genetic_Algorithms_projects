{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_task_Junior_DS.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP6Pf9mSa3mD"
      },
      "source": [
        "# Тестовое задание Junior Data Scientist\n",
        "\n",
        "Выполнил кандидат: Кудюров Владислав\n",
        "\n",
        "Использовал датасет на русском языке(corpus вопросов и ответов собирал сам), к датасету длбавил данные  прототипа диалогов,т.к. диалогов не достаточно для  оценки качества модели. Создан список произвольных вопросов из прототипа диалогов.\n",
        "\n",
        "Архитектуру нейронной сети и параметров подбирал сам.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUJ3vHGDIKBX"
      },
      "source": [
        "# Проверем видеокарту\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBz7ABgid0Ys"
      },
      "source": [
        "# Задание № 1\n",
        "\n",
        "По ссылке ниже находится документ, в котором приведены примеры 10 реальных диалогов оператора с клиентом. Используя диалоги в качестве прототипа, необходимо создать блок-схему голосового чат-бота, который будет совершать обзвон базы номеров. Ключевой задачей чат-бота является конверсия холодных обзвонов в лидов (передача лида техническому специалисту, который закроет сделку)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjCHlihgIizA"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras\n",
        "from keras.utils import plot_model\n",
        "import numpy as np \n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Flatten \n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Adadelta \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras import utils \n",
        "from tensorflow.keras.utils import Sequence, to_categorical, plot_model \n",
        "import yaml \n",
        "from matplotlib import pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebss7hLeJ3se"
      },
      "source": [
        "# Подключаем гугл диск\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y57-7PXsTMcy"
      },
      "source": [
        "# Загружаем данные и обрабатываем их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiUkKaijK7S8"
      },
      "source": [
        "# Открываем файл с диалогами\n",
        "\n",
        "corpus = open('/content/drive/My Drive/datasets/Диалоги2(рассказы).yml', 'r') # открываем файл с диалогами в режиме чтения\n",
        "document = yaml.safe_load(corpus) # загружаем файл *глоссарий\n",
        "conversations = document['разговоры'] # загружаем диалоги из файла и заносим в conversations \n",
        "print('Количество пар вопрос-ответ : {}'.format(len(conversations)))\n",
        "print('Пример диалога : {}'.format(conversations[11906]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Mb9MdKJ35D"
      },
      "source": [
        "# Отделение знаков препинания пробелами\n",
        "def replaceSigns(s):\n",
        "    s = s.replace('.', ' . ')\n",
        "    s = s.replace(',', ' , ')\n",
        "    s = s.replace('!', ' ! ')\n",
        "    s = s.replace('?', ' ? ')\n",
        "    return s.lower()\n",
        "\n",
        "# Исправленная версия\n",
        "def strToTokens(sentence: str, maxlen=14): # функция принимает строку на вход (предложение с вопросом)\n",
        "    words = sentence.lower().split() # приводит предложение к нижнему регистру и разбирает на слова\n",
        "    tokensList = tokenizer.texts_to_sequences([words])\n",
        "    return pad_sequences(tokensList, maxlen=maxlen , padding='post')\n",
        "\n",
        "# Класс строит график всего обучения, состоявшего из нескольких этапов.\n",
        "class LearnHistory:\n",
        "    def __init__(self):\n",
        "        self.history = {'loss':[], 'val_loss': []}\n",
        "\n",
        "    def append(self, history):\n",
        "        self.history['loss'].extend(history.history['loss'])\n",
        "        self.history['val_loss'].extend(history.history['val_loss'])\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        ep = [i+1 for i in range(len(self.history['loss']))]\n",
        "        plt.plot(ep, self.history['loss'], label='Loss на обучающем наборе')\n",
        "        plt.plot(ep, self.history['val_loss'], label='Loss на проверочном наборе')\n",
        "        plt.ylabel('Средняя ошибка')\n",
        "        plt.xlabel('Эпоха')\n",
        "        if ylim is not None:\n",
        "            plt.ylim(0, ylim)\n",
        "        plt.title('История обучения')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Делает батчи для обучения\n",
        "class MySequence(Sequence):\n",
        "    def __init__(self, x_set, y_set, voc_len, batch_size=50):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.voc_len = voc_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_z = np.zeros((self.batch_size, batch_y.shape[1], self.voc_len), dtype=np.int32)\n",
        "        for n in range(self.batch_size):\n",
        "            for j in range(batch_y.shape[1] - 1):\n",
        "                batch_z[n, j, batch_y[n, j+1]] = 1\n",
        "        return [batch_x, batch_y], batch_z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17kMY-RKKVlC"
      },
      "source": [
        "# Тестовые вопросы\n",
        "myQuestions = [\n",
        "               'Удобно разговаривать?',\n",
        "               'У нас для вас предложение?', \n",
        "               'Мы помогаем экономить до 40% от суммы расходов на электроэнергию с гарантией результата, интересует?',\n",
        "               'Скажите, вас заинтересовала бы возможность экономии до 40% расходов на оплату электроенергии?', \n",
        "               'Предлагаю детальнее вам ознакомиться с нашим предложением?',\n",
        "               'Почему?',\n",
        "               'Разве вам не хотелось бы экономить существенную часть расходов и направить эти средства на что-то более полезное?'\n",
        "               ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f9lI5AVVtL8"
      },
      "source": [
        "# Тестирование inference-модели на вопросах\n",
        "def testModel(encModel, decModel, questions, tokenizer, maxlen):\n",
        "    for question in questions:\n",
        "        question = replaceSigns(question)\n",
        "        statesValues = encModel.predict(strToTokens(question))\n",
        "        emptyTargetSeq = np.zeros((1, 1))    \n",
        "        emptyTargetSeq[0, 0] = tokenizer.word_index['start']\n",
        "        decodedTranslation = ''\n",
        "        for i in range(maxlen): \n",
        "            decOutputs, h, c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "            ind = np.argmax(decOutputs, axis=-1)[0, 0]\n",
        "            if ind == 0:\n",
        "                break\n",
        "            sampledWord = tokenizer.index_word[ind]\n",
        "            if sampledWord == 'end':\n",
        "                break\n",
        "            decodedTranslation += (' ' + sampledWord)\n",
        "            emptyTargetSeq[0, 0] = ind \n",
        "            statesValues = [h, c] \n",
        "        \n",
        "        print('Question: ' + question)\n",
        "        print('Answer: ' + decodedTranslation)\n",
        "\n",
        "# Для задания расписания для learning rate\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l9D_0aXVtV0"
      },
      "source": [
        "# Разбираем вопросы-ответы с проставлением тегов ответам\n",
        "######################\n",
        "# Собираем вопросы и ответы в списки\n",
        "questions = list() # здесь будет список вопросов\n",
        "answers = list() # здесь будет список ответов\n",
        "\n",
        "# В каждом диалоге берем фразу и добавляем в лист\n",
        "# Если в ответе не одна фраза - то сцепляем сколько есть\n",
        "for con in conversations: # для каждой пары вопрос-ответ\n",
        "  if len(con) > 2 : # если ответ содержит более двух предложений (кол-во реплик, кол-во вариантов ответа)\n",
        "    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов\n",
        "    replies = con[1:] # а ответную составляем из последующих строк\n",
        "    ans = '' # здесь соберем ответ\n",
        "    for rep in replies: # каждую реплику в ответной реплике\n",
        "      ans += ' ' + rep \n",
        "    answers.append(ans) #добавим в список ответов\n",
        "  elif len(con)> 1: # если на 1 вопрос приходится 1 ответ\n",
        "    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов\n",
        "    answers.append(con[1]) # а ответную в список ответов\n",
        "\n",
        "# Очищаем строки с неопределенным типов ответов\n",
        "answersCleaned = list()\n",
        "for i in range(len(answers)):\n",
        "  if type(answers[i]) == str:\n",
        "    answersCleaned.append(answers[i]) #если тип - строка, то добавляем в ответы\n",
        "  else:\n",
        "    questions.pop(i) # если не строка, то ответ не добавился, и плюс убираем соответствующий вопрос\n",
        "\n",
        "# Сделаем теги-метки для начала и конца ответов\n",
        "answers = list()\n",
        "for i in range(len(answersCleaned)):\n",
        "  answers.append( '<START> ' + answersCleaned[i] + ' <END>' )\n",
        "\n",
        "# Выведем обновленные данные на экран\n",
        "print('Вопрос : {}'.format(questions[2000]))\n",
        "print('Ответ : {}'.format(answers[2000]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9UTaZZaVyBi"
      },
      "source": [
        "# Отделяем знаки препинания от слов\n",
        "for i, s in enumerate(questions):\n",
        "    if isinstance(s, list):\n",
        "        s = s[0]\n",
        "    questions[i] = replaceSigns(s)\n",
        "\n",
        "for i, s in enumerate(answers):\n",
        "    if isinstance(s, list):\n",
        "        s = s[0]\n",
        "    answers[i] = replaceSigns(s)\n",
        "\n",
        "# Добавили oov_token, убрали знаки препинания из фильтра\n",
        "tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', oov_token='unknown')\n",
        "tokenizer.fit_on_texts(questions + answers) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности\n",
        "vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря\n",
        "vocabularySize = len(vocabularyItems)+1 # размер словаря\n",
        "print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))\n",
        "print( 'Размер словаря : {}'.format(vocabularySize))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bq478XjVyEY"
      },
      "source": [
        "# Устанавливаем закодированные входные данные(вопросы)\n",
        "######################\n",
        "tokenizedQuestions = tokenizer.texts_to_sequences(questions) # разбиваем текст вопросов на последовательности индексов\n",
        "maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) # уточняем длину самого большого вопроса\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие вопросы\n",
        "paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "encoderForInput = np.array(paddedQuestions) # переводим в numpy массив\n",
        "print('Пример оригинального вопроса на вход : {}'.format(questions[100])) \n",
        "print('Пример кодированного вопроса на вход : {}'.format(encoderForInput[100])) \n",
        "print('Размеры закодированного массива вопросов на вход : {}'.format(encoderForInput.shape)) \n",
        "print('Установленная длина вопросов на вход : {}'.format(maxLenQuestions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkqaa7tsWPBM"
      },
      "source": [
        "# Устанавливаем раскодированные входные данные(ответы)\n",
        "\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
        "maxLenAnswers = max([len(x) for x in tokenizedAnswers]) # уточняем длину самого большого ответа\n",
        "\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "decoderForInput = np.array(paddedAnswers) # переводим в numpy массив\n",
        "print('Пример оригинального ответа на вход: {}'.format(answers[100])) \n",
        "print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100])) \n",
        "print('Размеры раскодированного массива ответов на вход : {}'.format(decoderForInput.shape)) \n",
        "print('Установленная длина ответов на вход : {}'.format(maxLenAnswers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRs5luNKTchl"
      },
      "source": [
        "# Данные готовы к обучению."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO1RPvcgWPEP"
      },
      "source": [
        "# Перемешаем данные\n",
        "inds = np.array([i for i in range(11900)])\n",
        "np.random.shuffle(inds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zoxjco2WPG8"
      },
      "source": [
        "# Обучающие наборы\n",
        "x_train = paddedQuestions[inds[:10000]]\n",
        "y_train = paddedAnswers[inds[:10000]]\n",
        "x_val = paddedQuestions[inds[10000:10900]]\n",
        "y_val = paddedAnswers[inds[10000:10900]]\n",
        "x_test = paddedQuestions[inds[10900:11900]]\n",
        "y_test = paddedAnswers[inds[10900:11900]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW-aVNHKWPJ6"
      },
      "source": [
        "# сиквенсы для обучения - выдают батчи по 50\n",
        "seq_train = MySequence(x_train, y_train, voc_len=vocabularySize, batch_size=50)\n",
        "seq_val = MySequence(x_val, y_val, voc_len=vocabularySize, batch_size=50)\n",
        "seq_test = MySequence(x_test, y_test, voc_len=vocabularySize, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8etUd3OTjl6"
      },
      "source": [
        "# Параметры и протип простенькой модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1ieGYuIWPMh"
      },
      "source": [
        "DIM = 200\n",
        "\n",
        "# Первый входной слой, кодер, выходной слой\n",
        "encoderInputs = Input(shape=(None, )) # размеры на входе сетки (здесь будет encoderForInput)\n",
        "\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "encoderEmbedding = Embedding(vocabularySize, DIM,  mask_zero=True) (encoderInputs)\n",
        "\n",
        "# Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c\n",
        "# Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже\n",
        "encoderOutputs, state_h , state_c = LSTM(DIM, return_state=True)(encoderEmbedding)\n",
        "encoderStates = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeDwdJMpVyHq"
      },
      "source": [
        "# Второй входной слой, декодер, выходной слой\n",
        "decoderInputs = Input(shape=(None, )) # размеры на входе сетки (здесь будет decoderForInput)\n",
        "\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "# mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: \"У меня все хорошо PAD PAD PAD PAD PAD PAD..\"\n",
        "decoderEmbedding = Embedding(vocabularySize, DIM, mask_zero=True) (decoderInputs)\n",
        "\n",
        "# Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c\n",
        "decoderLSTM = LSTM(DIM, return_state=True, return_sequences=True)\n",
        "decoderOutputs , _ , _ = decoderLSTM(decoderEmbedding, initial_state=encoderStates)\n",
        "\n",
        "# И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе\n",
        "decoderDense = Dense(vocabularySize, activation='softmax') \n",
        "output = decoderDense (decoderOutputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK8zSKUzWaW6"
      },
      "source": [
        "# Собираем тренировочную модель нейросети\n",
        "\n",
        "model = Model([encoderInputs, decoderInputs], output)\n",
        "lh = LearnHistory()\n",
        "print(model.summary())\n",
        "\n",
        " # выведем на экран информацию о построенной модели нейросетии построим график для визуализации слоев и связей между ними\n",
        "plot_model(model, to_file='model.png', show_shapes=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwcFq7otWaZ9"
      },
      "source": [
        "# Определим модель кодера, на входе далее будут закодированные вопросы(encoderForInputs), на выходе состояния state_h, state_c\n",
        "encoderModel = Model(encoderInputs, encoderStates) \n",
        "\n",
        "decoderStateInput_h = Input(shape=(DIM,)) # обозначим размерность для входного слоя с состоянием state_h\n",
        "decoderStateInput_c = Input(shape=(DIM,)) # обозначим размерность для входного слоя с состоянием state_c\n",
        "\n",
        "decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs\n",
        "\n",
        "# Берём ответы, прошедшие через эмбединг, вместе с состояниями и подаём LSTM cлою\n",
        "\n",
        "decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)\n",
        "\n",
        "decoderStates = [state_h, state_c] # LSTM даст нам новые состояния\n",
        "decoderOutputs = decoderDense(decoderOutputs) # и ответы, которые мы пропустим через полносвязный слой с софтмаксом\n",
        "\n",
        "# Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния\n",
        "# на выходе предсказываемый ответ и новые состояния\n",
        "decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UISNYg27TrIG"
      },
      "source": [
        "# Модель. До и после обучения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2VwbKJHY2yz"
      },
      "source": [
        "# Необученная модель:\n",
        "testModel(encoderModel, decoderModel, myQuestions, tokenizer, maxLenAnswers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiUSrNEHWp4g"
      },
      "source": [
        "# Обучение на 50 эпохах\n",
        "model.compile(optimizer=Adam(lr=0.0002), loss='categorical_crossentropy', metrics='accuracy')\n",
        "history = model.fit(seq_train, validation_data=seq_val, epochs=50) \n",
        "lh.append(history)\n",
        "lh.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8uyYrHpaRij"
      },
      "source": [
        "После 100 эпохах обучения, модель обученная на датасете отвечает на произвольные вопросы из возможных диалогов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l3gsqJUWqDL"
      },
      "source": [
        "# После 100 эпох:\n",
        "lh.plot()\n",
        "testModel(encoderModel, decoderModel, myQuestions, tokenizer, maxLenAnswers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y9qpEpGWadG"
      },
      "source": [
        "# Создаем рабочую модель для вывода ответов на запросы пользователя\n",
        "\n",
        "def makeInferenceModels():\n",
        "  encoderModel = Model(encoderInputs, encoderStates) \n",
        "  decoderStateInput_h = Input(shape=(200 ,)) \n",
        "  decoderStateInput_c = Input(shape=(200 ,)) \n",
        "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c]\n",
        "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)\n",
        "  decoderStates = [state_h, state_c] \n",
        "  decoderOutputs = decoderDense(decoderOutputs)\n",
        "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
        "  return encoderModel , decoderModel\n",
        "\n",
        "\n",
        "# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов\n",
        "def strToTokens(sentence: str): \n",
        "  words = sentence.lower().split()\n",
        "  tokensList = list()\n",
        "  for word in words:\n",
        "    tokensList.append(tokenizer.word_index[word])\n",
        "  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')\n",
        "\n",
        "# Устанавливаем окончательные настройки и запускаем модель\n",
        "encModel, decModel = makeInferenceModels()\n",
        "for _ in range(6):\n",
        "  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
        "  emptyTargetSeq = np.zeros((1, 1))    \n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start']\n",
        "  stopCondition = False \n",
        "  decodedTranslation = ''\n",
        "  while not stopCondition :\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "    \n",
        "    sampledWordIndex = np.argmax(decOutputs, axis=-1) \n",
        "    sampledWord = None \n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: \n",
        "        decodedTranslation += ' {}'.format(word) \n",
        "        sampledWord = word \n",
        "\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True \n",
        "\n",
        "    emptyTargetSeq = np.zeros((1, 1)) \n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex \n",
        "    statesValues = [h, c] \n",
        "  \n",
        "  print(decodedTranslation[:-3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKm45fuYRq6P"
      },
      "source": [
        "Так как, в данном датасете, не хватает диалогов о предложении и продажах, чат-бот не может ответить корректно на вопросы. Для дальнейшего улучшения ответов или вопрос требуется правильно подобрать корпус с диалогами, а также увеличить колличество эпох обучения  и подборку параметров для снижении ошибки на тестовых вопросах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSnI_AsapdZS"
      },
      "source": [
        "# Попробуем усовершенствовать данную модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MyYh6OQUZW8"
      },
      "source": [
        "#  Наверное наибольшую ценность имеет обучение сети отвечать на произвольные вопросы, для этого надо уменьшать потери на контрольной выборке.\n",
        "# Разобьем все данные, как положено по феншую, на тренировочные, валидационные и тестовые.\n",
        "\n",
        "# Перемешаем данные\n",
        "inds = np.array([i for i in range(11900)])\n",
        "np.random.shuffle(inds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyh619GiXnUm"
      },
      "source": [
        "# Пересоздадим выборки\n",
        "x_train = paddedQuestions[inds[:10000]]\n",
        "y_train = paddedAnswers[inds[:10000]]\n",
        "x_val = paddedQuestions[inds[10000:10900]]\n",
        "y_val = paddedAnswers[inds[10000:10900]]\n",
        "x_test = paddedQuestions[inds[10900:11900]]\n",
        "y_test = paddedAnswers[inds[10900:11900]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6RbWsgGXovS"
      },
      "source": [
        "vocabularySize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDrFmhzOXoyP"
      },
      "source": [
        "# Увеличим размерность векторного (embedding) пространства. Добавим дропауты в LSTM для борьбы с переобучением.\n",
        "# Перейдем на оптимизатор Adam\n",
        "DIM = 256\n",
        "\n",
        "encoderInputs = Input(shape=(None, ))\n",
        "encoderEmbedding = Embedding(vocabularySize, DIM,  mask_zero=True) (encoderInputs)\n",
        "encoderOutputs, state_h , state_c = LSTM(DIM, dropout=0.2, recurrent_dropout=0.2, return_state=True)(encoderEmbedding)\n",
        "encoderStates = [state_h, state_c]\n",
        "\n",
        "decoderInputs = Input(shape=(None, ))\n",
        "decoderEmbedding = Embedding(vocabularySize, DIM, mask_zero=True) (decoderInputs) \n",
        "decoderLSTM = LSTM(DIM, dropout=0.2, recurrent_dropout=0.2, return_state=True, return_sequences=True)\n",
        "decoderOutputs , _ , _ = decoderLSTM(decoderEmbedding, initial_state=encoderStates)\n",
        "decoderDense = Dense(vocabularySize, activation='softmax') \n",
        "output = decoderDense (decoderOutputs)\n",
        "\n",
        "model1 = Model([encoderInputs, decoderInputs], output)\n",
        "lh = LearnHistory() \n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFOE0zitXo1A"
      },
      "source": [
        "plot_model(model1, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNIrPIPFX_Nd"
      },
      "source": [
        "# Нам нужны 3 сиквенса на каждый тип данных.\n",
        "\n",
        "seq_train = MySequence(x_train, y_train, voc_len=vocabularySize, batch_size=100)\n",
        "seq_val = MySequence(x_val, y_val, voc_len=vocabularySize, batch_size=100)\n",
        "seq_test = MySequence(x_test, y_test, voc_len=vocabularySize, batch_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE9XQGpMX_QV"
      },
      "source": [
        "# Пробовал менять lr по расписанию.\n",
        "# learning_rate = CustomSchedule(model)\n",
        "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "history = model1.fit(seq_train, validation_data=(seq_val), epochs=20) \n",
        "lh.append(history)\n",
        "lh.plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Z2r1SSoHO6"
      },
      "source": [
        "# Выводы:\n",
        "\n",
        "- Для улучшения качества модели потребуется грамотно составить корпус;\n",
        " \n",
        "- Увеличить колличество эпох.\n",
        "\n",
        "Создание чат-бота подразумевается, что выходная последовательность должна как-то достаточно однозначно вытекать из входной. Создание чат-бота - скорее задача классификации. Особенность в том, что классов очень много (тысячи). Надо найти в базе запрос, максимально похожий (а лучше совпадающий) с введенным запросом и рандомно выдать один из заготовленных ответов.\n",
        "\n",
        "Нейронная сеть является универсальным аппроксиматором, и есть теорема, утверждающая, что сеть способна обеспечить любую наперед заданную точность. Но это касается лишь обучаемых данных. Поэтому задача снижения потерь на тренировочных данных не слишком трудна, нужно лишь взять досточно сложную сеть и нужное число эпох. Ценность такого обучения весьма ограничена - сеть лишь запомнит конкретные ответы на конкретные вопросы, но не сможет хорошо отвечать на произвольные вопросы.\n",
        "\n",
        "Более ценным результатом было бы обучение, дающее малую потерю на тестовом наборе. Однако в нашем случае добиться хорошего результата на тестовом наборе не получилось - несмотря на все предпринятые меры, очень быстро наступало переобучение, а потери оставались на высоком уровне. Я объясняю такой результат тем, что данные в ответах не вытекают логически из вопросов, и довольно произвольны. \n",
        "\n",
        "При обучении модели возникли серьезные проблемы с нехваткой памяти, так как OHE кодировка давала огромные массивы, приводившие к краху сессии. Поэтому мне пришлось создать свой сиквенс, выдающий данные в OHE формате батчами лишь по 50 штук. Это решило проблему.\n",
        "\n",
        "\n"
      ]
    }
  ]
}